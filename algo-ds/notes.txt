non-sorted lists aren't intuitive to machines
    search algos are used on [] with all sorts of data

binary search
    does not return target value
    returns position in the list the target occupies
    the input list must be sorted
    tree breakdown
        input: sorted list
        output: position in the list of the target value or DNE

algo traits
    steps in order
    steps are distinct
    produces result
    completes in finite time

algo defined
    1: determine middle position of the sorted list
    2: compare the element in the middle position to target element
    3: if match, return to middle position and end
    4: if not, check if middle element is smaller than target element if so, go back to step 2
    5: if greater, check from start to end of the new list
    6... repeat until found or until sub-list contains only one element. if single element sub-list does not contain the target value end algo

efficiency of binary search
    worst case scenario is used as a benchmark
    increasing target value number shows a clear discrepancy between linear and binary in number of tries (n)

big-O 
    theoretical definition of the complexity of an algo as a function of the size
    O(n)
        notation used to describe complexity
        order of magnitude of complexity
        a function of the size
        upper bounds of the algorithm

    complexity is relative
        thus we compare complexity of algorithms to other related algorithms

    linear search
        O(n)
    binary search
        O(log n)

common complexity
    linear search

O(1) constant time

O(log n)    
    log2 of n + 1
        logarithmic runtime
        sublinear

O(n)
    linear time

polynomial runtime
    O(n^2)
        quadratic time
    O(n^3)
        cubic runtime

O(n log n)
    quasi-linear runtime

exponential runtimes
    O(x^n)
    brute force

traveling salesman
    O(n!)
    given list of cities and the distance between each pair what is the shortest route that visits each city and returns to the origin city
    the number of cities dramatically increases the amount of options
    the mathematical relationship that defines this is called a factorio = n!
    n! = n(n-1)(n-2)...(2)(1)
    3! = 3 x 2 x 1 = 6
    basically n x n-1 until n = 1
    factorial/combinatorial runtime
    low values of n may use this runtime but with high n values like 200 it will take way an insurmountable amount of time to solve the problem
    
determining the worst runtime of an algorithm
    binary search
        assuming list is sorted
            determine middle position, since many languages hold the size no need to go through the list to determine it (constant time) O(1)
                if this wasn't the case each item would need to be counted one by one (linear time)
            compare the element in the middle position to target (constant time) O(1)
            best case is if middle position is target value thus best case is (constant time)
            if not matched split into sublists until single element sublist is equal to the target value (logarithmic runtime) {since we discard half the values each time}
        the time complexity of an algorithm will be based on its worst case scenario runtime which in this case is logarithmic O(log n)
    

        

            
            

    


wait what is n? 









    






